# MCAQ-YOLO: Morphological Complexity-Aware Quantization for YOLO

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Official implementation of **MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning**

## ğŸ“‹ Overview

MCAQ-YOLO introduces a novel spatial quantization framework for object detection that dynamically allocates bit precision based on morphological complexity. By analyzing local visual characteristics through five complementary metrics (fractal dimension, texture entropy, gradient variance, edge density, and contour complexity), the framework achieves superior detection accuracy with aggressive compression ratios.

### Key Features

- **Morphological Complexity Analysis**: Multi-metric assessment of spatial regions for informed bit allocation
- **Curriculum Learning**: Progressive training strategy for stable optimization
- **Spatial Adaptive Quantization**: Tile-wise mixed-precision with smooth transitions
- **Hardware-Aware Design**: Optimized for modern accelerators with kernel fusion

### Performance Highlights

- **3.5% mAP improvement** over uniform 4-bit quantization
- **7.6Ã— model compression** with minimal accuracy loss
- **40% faster convergence** with curriculum learning
- **Strong correlation** (Ï=0.89) between complexity and quantization sensitivity

## ğŸš€ Quick Start

### Installation
```bash
# Clone the repository
git clone https://github.com/yooooonjae/mcaq-yolo.git
cd mcaq-yolo

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install package
pip install -e .
```

### Training
```bash
# Train with default configuration
mcaq-yolo-train --config configs/train_config.yaml

# Train with custom settings
mcaq-yolo-train \
    --config configs/train_config.yaml \
    --device cuda:0 \
    --output-dir outputs/experiment_1

# Resume from checkpoint
mcaq-yolo-train \
    --config configs/train_config.yaml \
    --resume outputs/experiment_1/latest.pth
```

### Inference
```bash
# Single image inference
mcaq-yolo-infer \
    --model outputs/best.pth \
    --source image.jpg \
    --visualize

# Batch inference on directory
mcaq-yolo-infer \
    --model outputs/best.pth \
    --source /path/to/images \
    --save-dir results
```

## ğŸ“Š Model Architecture
```
MCAQ-YOLO
â”œâ”€â”€ YOLOv8 Backbone
â”œâ”€â”€ Morphological Complexity Analyzer
â”‚   â”œâ”€â”€ Fractal Dimension
â”‚   â”œâ”€â”€ Texture Entropy (LBP)
â”‚   â”œâ”€â”€ Gradient Variance
â”‚   â”œâ”€â”€ Edge Density
â”‚   â””â”€â”€ Contour Complexity
â”œâ”€â”€ Complexity-to-Bit Mapping Network
â”‚   â””â”€â”€ Learnable monotonic mapping
â””â”€â”€ Spatial Adaptive Quantization
    â””â”€â”€ Tile-wise mixed precision
```

## ğŸ¯ Training Configuration

Create a configuration file `configs/train_config.yaml`:
```yaml
# Model configuration
model:
  name: yolov8n
  pretrained: true
  teacher_path: yolov8n.pt

# Dataset configuration
data:
  train_path: /path/to/train
  val_path: /path/to/val
  img_size: 640
  num_workers: 8

# Training configuration
epochs: 300
batch_size: 16
learning_rate: 0.001

# Quantization configuration
quantization:
  min_bits: 2
  max_bits: 8
  target_bits: 4.0

# Curriculum learning
curriculum:
  enabled: true
  warmup_epochs: 30
  initial_complexity: 0.2
  initial_temperature: 10.0
  type: exponential

# Optimizer
optimizer:
  type: adamw
  weight_decay: 0.05

# Learning rate scheduler
scheduler:
  type: cosine

# Loss weights
loss:
  lambda_bit: 0.01
  lambda_smooth: 0.001
  lambda_kd: 0.5
  lambda_reg: 0.0001

# Training settings
training:
  grad_clip: 1.0
  save_interval: 10
  eval_interval: 5

# Hardware
device: cuda
output_dir: outputs
```

## ğŸ“ˆ Results

### Quantization Performance

| Method | Bits | mAP@0.5 | mAP@0.5:0.95 | Size (MB) | FPS |
|--------|------|---------|--------------|-----------|-----|
| YOLOv8-FP32 | 32 | 89.3% | 68.1% | 108.3 | 92 |
| Uniform-4bit | 4 | 82.1% | 58.3% | 13.5 | 156 |
| **MCAQ-YOLO** | 4.2 | **85.6%** | **63.2%** | 14.2 | 151 |

### Complexity Analysis

| Class | Complexity | Allocated Bits | mAP Drop (3-bit) |
|-------|------------|----------------|------------------|
| Person | 0.72 | 5.8 | 17.2% |
| Helmet | 0.25 | 4.1 | 5.3% |
| Background | 0.21 | 3.8 | 2.1% |

## ğŸ”§ Advanced Usage

### Custom Morphological Metrics
```python
from mcaq_yolo.core.morphology import MorphologicalComplexityAnalyzer

# Create custom analyzer
analyzer = MorphologicalComplexityAnalyzer(
    tile_sizes=[8, 16, 32],
    cache_size=2000,
    device='cuda'
)

# Compute complexity for your data
complexity_map = analyzer(features)
```

### Bit Allocation Policies
```python
from mcaq_yolo.core.bit_allocation import AdaptiveBitAllocation

# Use different allocation policies
allocator = AdaptiveBitAllocation(
    min_bits=2,
    max_bits=8,
    target_bits=4.0,
    policy='exponential'  # 'linear', 'exponential', 'threshold', 'learned'
)
```

### Curriculum Learning Strategies
```python
from mcaq_yolo.core.curriculum import CurriculumScheduler

# Configure curriculum
curriculum = CurriculumScheduler(
    warmup_epochs=30,
    total_epochs=300,
    curriculum_type='cosine'  # 'linear', 'exponential', 'cosine', 'step'
)
```

## ğŸ“ Project Structure
```
mcaq_yolo/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ morphology.py       # Complexity analysis
â”‚   â”œâ”€â”€ bit_allocation.py   # Bit mapping network
â”‚   â”œâ”€â”€ quantization.py     # Spatial quantization (Updated with CUDA support)
â”‚   â””â”€â”€ curriculum.py       # Curriculum learning
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mcaq_yolo.py        # Main model
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ops/                    # (New) CUDA Kernel & Ops
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ mcaq_kernel.cu  # (New) CUDA Kernel implementation
â”‚       â””â”€â”€ mcaq_ops.cpp    # (New) C++ Binding for PyTorch
â”œâ”€â”€ engine/                 # (New) TensorRT Plugin
â”‚   â””â”€â”€ MCAQPlugin.cpp      # (New) TensorRT Plugin implementation
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dataset.py          # Data utilities
â”‚   â”œâ”€â”€ evaluation.py       # Metrics
â”‚   â”œâ”€â”€ visualization.py    # Plotting
â”‚   â””â”€â”€ model_utils.py
â”œâ”€â”€ configs/                # Configuration files
â”‚   â””â”€â”€ train_config.yaml
â”œâ”€â”€ train.py                # Training script
â”œâ”€â”€ inference.py            # Inference script
â””â”€â”€ setup.py                # (Modified) Build script for CUDA extensions
```

## ğŸ“ Citation

If you use MCAQ-YOLO in your research, please cite:
```bibtex
@article{mcaqyolo2025,
  title={MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning},
  author={Seo, Yoonjae and Elbasani, E. and Lee, Jaehong},
  journal={arXiv preprint arXiv:2511.12976},
  year={2025}
}
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- YOLOv8 by Ultralytics
- PyTorch team for the deep learning framework
- Open source community for valuable feedback

## ğŸ“§ Contact

- **Corresponding Author**: Jaehong Lee (jlee@sejong.ac.kr)
- **First Author**: Yoonjae Seo (22013378@sju.ac.kr)
- **Second Author**: E. Elbasani (ermal.elbasani@sejong.ac.kr)

---

**Note**: This is research code. While we strive for reliability, please use with appropriate caution in production environments.
